{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Churn Prediction with PySpark\n\nDetect and predict customer churn with machine learning models is a common problem Data Scientists are often confronted with in a customer-facing business. This project will serve as an exploration of how to make a churn-prediction model using PySpark, with the following steps included:\n* explore and manipulate our dataset\n* engineer relevant features for our problem\n* split data into train and test sets by sampling churn\n* train binary classifier models with Spark’s DataFrame-based MLlib\n* select and fine-tune the final model with Spark’s ML Pipelines and a StratifiedCrossValidator\n* Evaluation of Prediction Performance (Metric: F1 Score)\n\nIn Part I we only use a subset of data (128MB) to train our churn prediction models locally with Spark. In order to use the full dataset (12GB) for model training, check the Part II, where we deploy a cluster on a cloud service."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[{"output_type":"stream","text":"Collecting pyspark\n  Downloading pyspark-3.0.2.tar.gz (204.8 MB)\n\u001b[K     |████████████████████████████████| 204.8 MB 50 kB/s s eta 0:00:01    |███████▊                        | 49.5 MB 22.4 MB/s eta 0:00:07     |██████████████▉                 | 94.6 MB 48.5 MB/s eta 0:00:03     |█████████████████▊              | 113.1 MB 81.0 MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |████████████████████████████████| 198 kB 42.1 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25l|","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nimport time\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = None\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql import Window\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import feature as FT\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up a Spark session\nspark = SparkSession \\\n        .builder \\\n        .appName('Sparkify Churn Prediction') \\\n        .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check, if Spark session was setup correctly\nspark.sparkContext.getConf().getAll()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About the Dataset\nWe will use the user events logs (duration about 2 months) of a music-streaming service called Sparkify as our dataset. With these logs we can predict whether this user is more likely to stay or is more likely to churn. The dataset contains 286,500 rows and 18 features:\n\n* artist: singer of a song\n* auth: login status\n* firstName: first name of the user\n* gender: gender of the user\n* itemInSession: number of the item in the current session\n* lastName: surname of the user\n* length: song length\n* level: whether a customer is paying for the service or not\n* location: location of the user\n* method: how a user is getting web pages\n* page: page browsing information\n* registration: time stamp of the regestration of the user\n* sessionId: session ID\n* song: name of a song\n* status: page return code\n* ts: timestamp of the log item\n* userAgent: browser client\n* userId: user ID"},{"metadata":{},"cell_type":"markdown","source":"# Load/Clean Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\ndf = spark.read.json('../input/mini-sparkify/mini_sparkify_event_data.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show number of rows\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show all columns and data types\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show first rows\ndf.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing (NaN) values in all columns\ncount_nan_cols = [F.count(F.when(F.isnan(c), c)).alias(c) for c in df.columns]\ndf.select(count_nan_cols).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No missing(NaN) values in all columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check Null values in all columns\ncount_null_cols = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]\ndf.select(count_null_cols).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Null values are found in columns related with user information and song information."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.filter(F.col('artist').isNull()).toPandas().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.filter(F.col('artist').isNull()!=True).toPandas().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Pages that are not NextSong will have null values for artist, length and song."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check empty columns per column\ncount_invalid_cols = [F.count(F.when(F.col(c)=='', c)).alias(c) for c in df.columns]\ndf.select(count_invalid_cols).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out rows with empty userId \ndf.filter(F.col('userId')=='').toPandas().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Users with empty userId are those who did not register and log in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with missing user id\ndf = df.where(df.userId!='')\n\n# drop duplicate rows if any exists\ndf = df.dropDuplicates()\n\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the Cancellation Confirmation event to define churn\nchurned_users = df.filter(F.col('page')=='Cancellation Confirmation')\n\nflag_churn = F.udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, T.IntegerType())\ndf = df.withColumn('churn', flag_churn('page'))\n\nchurned_users.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert Timestamps (ts) to Datetime\ndf = df.withColumn('reg_date', (F.col('registration')/1000).cast(T.TimestampType()))\ndf = df.withColumn('date', (F.col('ts')/1000).cast(T.TimestampType()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user-based observation start/end dates\nmin_date = df.agg({'date':'min'}).collect()[0]['min(date)']\nmax_date = df.agg({'date':'max'}).collect()[0]['max(date)']\nmin_reg_date = df.agg({'reg_date':'min'}).collect()[0]['min(reg_date)']\nmax_reg_date = df.agg({'reg_date':'max'}).collect()[0]['max(reg_date)']\n\nprint(f'min_date:{min_date}')\nprint(f'max_date:{max_date}')\nprint(f'min_reg_date:{min_reg_date}')\nprint(f'max_reg_date:{max_reg_date}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get first log date\nw = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\ndf = df.withColumn('first_date', F.first('date').over(w))\n\n# infer observation start date\ndf = df.withColumn('obs_start',\n                  (F.when(F.col('reg_date')<min_date, min_date)\n                    .when(F.col('reg_date')<F.col('first_date'), F.col('reg_date'))\n                    .otherwise(F.col('first_date')))\n                  )\n\n# infer observation end date\ndf = df.withColumn('obs_end',\n                  (F.when(F.last('churn').over(w)==1, F.last('date').over(w))\n                     .otherwise(max_date))\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# last subscription level\ndf = df.withColumn('last_level', F.last('level').over(w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get hour, weekday out of the date\ndf = df.withColumn('hour', F.date_format(F.col('date'), 'H'))\ndf = df.withColumn('weekday', F.date_format(F.col('date'), 'E'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user distribution per hour of the day\nusers_per_hour_pd = df.select(['userId', 'hour']).dropDuplicates().groupBy(['hour']).count().sort('hour').toPandas()\n\n# observe the distribution\nax = users_per_hour_pd.plot(x='hour', kind='bar', figsize=(10,5))\nax.get_legend().remove()\nplt.xlabel('\\nHour', fontsize=14)\nplt.ylabel('# Users', fontsize=14)\nplt.title('Users per hour', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user interaction per hour\ninteractions_per_hour_pd = df.select(['userId', 'hour']).groupBy(['hour']).count().sort('hour').toPandas()\n\n# plot the interactoins\nax = interactions_per_hour_pd.plot(x='hour', kind='bar', figsize=(10,5))\nax.get_legend().remove()\nplt.xlabel('\\nHour', fontsize=14)\nplt.ylabel('# Interactions', fontsize=14)\nplt.title('User interactions per hour', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user interactions per weekday\ninteractions_per_weekday_pd = df.select(['userId', 'weekday']).groupBy(['weekday']).count().sort('weekday').toPandas()\n\n# plot\nax = interactions_per_weekday_pd.plot(x='weekday', kind='bar', figsize=(10,5))\nax.get_legend().remove()\nplt.xlabel('\\nWeekday', fontsize=14)\nplt.ylabel('# Interactions', fontsize=14)\nplt.title('User interactions per weekday', fontsize=14)\nplt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# churn per weekday\nchurn_per_weekday_pd = df.select(['userId', 'weekday', 'churn']).groupby(['weekday']).sum().sort('weekday').toPandas()\n\n# plot\nax = churn_per_weekday_pd[['weekday','sum(churn)']].plot(x='weekday', kind='bar', figsize=(10,5))\nax.get_legend().remove()\nplt.xlabel('', fontsize=14)\nplt.ylabel('Churn rate', fontsize=14)\nplt.title('Churn per weekday', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of page visits\npage_visits_pd = df.groupBy('page').count().toPandas().sort_values('count')\n\n# plot all page events in the dataset:\nplt.figure(figsize=(15,8))\nsns.barplot(x='page', y='count', data=page_visits_pd, color='steelblue')\nplt.title('Page Visits', fontsize=14)\nplt.xticks(rotation=40)\nplt.xlabel('', fontsize=12)\nplt.ylabel('#Page Visits', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nNow that we've familiarized ourselves with the data set, we try to find and build promising features to train the model on:\n\n* aggregate all necessary columns by user\n* extract features from 'page' column which keeps track of pages a user visits\n* generate hourly statistics for each user\n* extract some duration-based features related with events"},{"metadata":{"trusted":true},"cell_type":"code","source":"# aggregation by user\nuser_df = df.groupby('userId').agg(\n    \n    # User-level features\n    F.max('churn').alias('churn'),\n    F.first('gender').alias('gender'),\n    F.first('reg_date').alias('reg_date'),\n    F.first('obs_start').alias('obs_start'),\n    F.first('obs_end').alias('obs_end'),\n    F.first('last_level').alias('last_level'),\n    \n    # interaction-level features (exclude Cancellation Confirmation)\n    F.count('page').alias('n_act'),\n    F.sum(F.when(F.col('page')=='About', 1).otherwise(0)).alias('n_about'),\n    F.sum(F.when(F.col('page')=='Add Friend', 1).otherwise(0)).alias('n_addFriend'),\n    F.sum(F.when(F.col('page')=='Add to Playlist', 1).otherwise(0)).alias('n_addToPlaylist'),\n    F.sum(F.when(F.col('page')=='Cancel', 1).otherwise(0)).alias('n_cancel'),\n    F.sum(F.when(F.col('page')=='Downgrade', 1).otherwise(0)).alias('n_downgrade'),\n    F.sum(F.when(F.col('page')=='Error', 1).otherwise(0)).alias('n_error'),\n    F.sum(F.when(F.col('page')=='Help', 1).otherwise(0)).alias('n_help'),\n    F.sum(F.when(F.col('page')=='Home', 1).otherwise(0)).alias('n_home'),\n    F.sum(F.when(F.col('page')=='Logout', 1).otherwise(0)).alias('n_logout'),\n    F.sum(F.when(F.col('page')=='NextSong', 1).otherwise(0)).alias('n_song'),\n    F.sum(F.when(F.col('page')=='Roll Advert', 1).otherwise(0)).alias('n_rollAdvert'),\n    F.sum(F.when(F.col('page')=='Save Settings', 1).otherwise(0)).alias('n_saveSettings'),\n    F.sum(F.when(F.col('page')=='Settings', 1).otherwise(0)).alias('n_settings'),\n    F.sum(F.when(F.col('page')=='Submit Downgrade', 1).otherwise(0)).alias('n_submitDowngrade'),\n    F.sum(F.when(F.col('page')=='Submit Upgrade', 1).otherwise(0)).alias('n_submitUpgrade'),\n    F.sum(F.when(F.col('page')=='Thumbs Down', 1).otherwise(0)).alias('n_thumbsDown'),\n    F.sum(F.when(F.col('page')=='Thumbs Up', 1).otherwise(0)).alias('n_thumbsUp'),\n    F.sum(F.when(F.col('page')=='Upgrade', 1).otherwise(0)).alias('n_upgrade'),\n    \n    # song-level features\n    F.countDistinct('artist').alias('n_artist'),\n    F.sum('length').alias('sum_length'),\n    \n    # session-level features\n    F.countDistinct('sessionId').alias('n_session'),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract new features from some aggregated statistics\nuser_df = (user_df.withColumn('reg_days', F.datediff('obs_end', 'reg_date'))\n                  .withColumn('obs_hours', (F.unix_timestamp('obs_end')-F.unix_timestamp('obs_start'))/3600)\n                  .withColumn('n_act_per_hour', F.col('n_act')/F.col('obs_hours'))\n                  .withColumn('n_about_per_hour', F.col('n_about')/F.col('obs_hours'))\n                  .withColumn('n_addFriend_per_hour', F.col('n_addFriend')/F.col('obs_hours'))\n                  .withColumn('n_addToPlaylist_per_hour', F.col('n_addToPlaylist')/F.col('obs_hours'))\n                  .withColumn('n_downgrade_per_hour', F.col('n_downgrade')/F.col('obs_hours'))\n                  .withColumn('n_error_per_hour', F.col('n_error')/F.col('obs_hours'))\n                  .withColumn('n_help_per_hour', F.col('n_help')/F.col('obs_hours'))\n                  .withColumn('n_home_per_hour', F.col('n_home')/F.col('obs_hours'))\n                  .withColumn('n_logout_per_hour', F.col('n_logout')/F.col('obs_hours'))\n                  .withColumn('n_song_per_hour', F.col('n_song')/F.col('obs_hours'))\n                  .withColumn('n_rollAdvert_per_hour', F.col('n_rollAdvert')/F.col('obs_hours'))\n                  .withColumn('n_saveSettings_per_hour', F.col('n_saveSettings')/F.col('obs_hours'))\n                  .withColumn('n_settings_per_hour', F.col('n_settings')/F.col('obs_hours'))\n                  .withColumn('n_submitDowngrade_per_hour', F.col('n_submitDowngrade')/F.col('obs_hours'))\n                  .withColumn('n_submitUpgrade_per_hour', F.col('n_submitUpgrade')/F.col('obs_hours'))\n                  .withColumn('n_thumbsDown_per_hour', F.col('n_thumbsDown')/F.col('obs_hours'))\n                  .withColumn('n_thumbsUp_per_hour', F.col('n_thumbsUp')/F.col('obs_hours'))\n                  .withColumn('n_upgrade_per_hour', F.col('n_upgrade')/F.col('obs_hours'))\n          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only use these variables\nuser_df = user_df.select('userId', 'churn', 'gender', 'last_level', 'sum_length', 'n_session', 'reg_days', 'obs_hours', \n                         'n_act_per_hour', 'n_about_per_hour', 'n_addFriend_per_hour', 'n_addToPlaylist_per_hour',\n                         'n_cancel', 'n_downgrade_per_hour', 'n_error_per_hour', 'n_help_per_hour',\n                         'n_home_per_hour', 'n_logout_per_hour', 'n_song_per_hour', 'n_rollAdvert_per_hour',\n                         'n_saveSettings_per_hour', 'n_settings_per_hour', 'n_submitDowngrade_per_hour',\n                         'n_submitUpgrade_per_hour', 'n_thumbsDown_per_hour', 'n_thumbsUp_per_hour', 'n_upgrade_per_hour'\n                        )\nuser_df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to pandas dataframe for easy visualization\nuser_pd = user_df.toPandas()\nuser_pd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observe the behavior for users who stayed vs users who churned\nplt.figure(figsize=(6,5))\nsns.countplot(x='churn', data=user_pd)\n# plt.savefig('dist_churn.png')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical columns\ncat_cols = user_pd.select_dtypes('object').columns.tolist()\ncat_cols.remove('userId')\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observe the distribution of categorical features\nplt.figure(figsize=(12,5))\n\nfor i in range(len(cat_cols)):\n    plt.subplot(1, 2, i+1)\n    plt.tight_layout()\n    sns.countplot(x=cat_cols[i], data=user_pd, hue='churn')\n    plt.legend(['Not Churned', 'Churned'])\n    plt.title(cat_cols[i])\n    plt.xlabel(' ')\n    \n# plt.savefig('dist_categorical.png')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numerical columns\nnum_cols = user_pd.select_dtypes(include=np.number).columns.tolist()\nnum_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function to plot correlation among columns\ndef plot_corr(cols, figsize=(10, 10), filename=None, df=user_pd):\n    plt.figure(figsize=figsize)\n    sns.heatmap(df[cols].corr(),square=True, cmap='YlGnBu', annot=True, vmin=-1, vmax=1)\n    plt.ylim(len(cols), 0)\n    if filename:\n        plt.savefig(filename)\n    plt.show();\n    \n# observe the correlation between numerical features\nplot_corr(num_cols, figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly correlated (>0.8) variable pairs(groups):\n\n* churn, obs_hours, n_cancel\n* sum_length, n_session\n* n_act_per_hour, n_addFriend_per_hour, n_addToPlaylist_per_hour, n_downgrade_per_hour, n_help_per_hour, n_home_per_hour, n_song_per_hour, n_thumbsUp_per_hour"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot highly correlated columns\ncols = ['n_act_per_hour', 'n_addFriend_per_hour', 'n_addToPlaylist_per_hour', 'n_downgrade_per_hour',\n        'n_help_per_hour', 'n_home_per_hour', 'n_song_per_hour', 'n_thumbsUp_per_hour']\nplot_corr(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# highly correlated columns\ndrop_cols = ['obs_hours', 'n_cancel', 'sum_length', 'n_act_per_hour', \n             'n_addFriend_per_hour','n_addToPlaylist_per_hour', \n             'n_downgrade_per_hour', 'n_help_per_hour','n_home_per_hour', \n             'n_thumbsUp_per_hour']\n\nnum_cols = [col for col in num_cols if col not in drop_cols]\n\n# observe the correlation between numerical features after removing highly correlated columns\nplot_corr(num_cols, figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observe the distribution of numerical features\nnum_cols.remove('churn')\n\nplt.figure(figsize=(12, 16))\n\nfor i in range(len(num_cols)):\n    plt.subplot(5,3,i+1)\n    plt.tight_layout()\n    sns.distplot(user_pd[user_pd['churn']==0][num_cols[i]],\n                 hist=False, norm_hist=True, kde_kws={'shade': True, 'linewidth': 2})\n    sns.distplot(user_pd[user_pd['churn']==1][num_cols[i]],\n                 hist=False, norm_hist =True, kde_kws={'shade': True, 'linewidth': 2})\n    plt.legend(['Not Churned','Churned'])\n    plt.title(num_cols[i])\n    plt.xlabel(' ')\n    plt.yticks([])\n\n# plt.savefig('dist_numerical.png')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the numerical features are skewed. \n\nThe range of values in a feature should reflect their importance. Higher values imply higher importances. Some features might have larger values than others and are required to be transformed for equal importance. There are two common methods to do feature scaling: \n\n1. **Normalization**: \nNormalize numerical features to range [0,1] e.g. via min-max normalization: Normalised Value = (Value - Feature Min)/(Feature Max - Feature Min)\n\n2. **Standardization**: \nThe Central Limit Theorem guarantees that the average of independent random variables is approximately normally distributed even when the individual random variables are not normally distributed. By standardization you ensure the values in a feature follow the normal distribution whereby mean of the values is 0 and standard deviation is 1. Standardized Value = (Value - Feature Mean)/Feature Standard Deviation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we have 15 features in total (excluding the userId and label(churn) columns)\nreverse_cols = [col for col in user_df.columns if col not in drop_cols]\nuser_df = user_df.select(*reverse_cols).withColumnRenamed('churn', 'label')\nuser_df.persist()\nuser_df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and test sets, sample by label\nratio = 0.7\ntrain = user_df.sampleBy('label', fractions={0:ratio, 1:ratio}, seed=123)\ntest = user_df.subtract(train)\n\nprint('train set:')\ntrain.groupBy('label').count().show()\nprint('test set:')\ntest.groupBy('label').count().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build data-process stages to encode, scale and assemble features\nstages = []\n\n# encode categorical features\nfor col in cat_cols:\n    indexer = FT.StringIndexer(inputCol=col, outputCol=col+'_idx')\n    encoder = FT.OneHotEncoder(inputCols=[indexer.getOutputCol()], outputCols=[col+'_vec'])\n    stages += [indexer, encoder]\n\n# scale numeric features via standartization so that they are closer to normal distribution\nfor col in num_cols: \n    assembler = FT.VectorAssembler(inputCols=[col], outputCol=col+'_vec')\n    scaler = FT.StandardScaler(inputCol=col+'_vec', outputCol=col+'_scl')\n    stages += [assembler, scaler]\n\n# assemble features  into a feature vector\ninput_cols = [c+'_vec' for c in cat_cols] + [c+'_scl' for c in num_cols]\nassembler = FT.VectorAssembler(inputCols=input_cols, outputCol='features')\nstages += [assembler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(maxIter=10)\ndtc = DecisionTreeClassifier(seed=2)\nrfc = RandomForestClassifier(seed=3)\n\npipelines = [\n    Pipeline(stages=stages+[lr]),\n    Pipeline(stages=stages+[dtc]),\n    Pipeline(stages=stages+[rfc]),\n]\n\nfor model, pipeline in zip([lr, dtc, rfc], pipelines):\n    print('\\n', type(model))\n    \n    # start training\n    start = time.time()\n    model = pipeline.fit(train)\n    end = time.time()\n    print(f'train time: {end-start:.0f}s')\n    \n    # make predictions\n    pred_train = model.transform(train)\n    pred_test = model.transform(test)\n    \n    # evaluate with F1-score which better suits for inbalanced dataset\n    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n    f1_score = evaluator.evaluate(pred_train, {evaluator.metricName: \"f1\"})\n    print(\"Training f1 score: {}\".format(f1_score))\n    f1_score = evaluator.evaluate(pred_test, {evaluator.metricName: \"f1\"})\n    print(\"Testing f1 score: {}\".format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Tuning with K-fold Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(classifier, param):\n    pipeline = Pipeline(stages=stages+[classifier])\n\n    model = CrossValidator(\n        estimator=pipeline,\n        estimatorParamMaps=param,\n        evaluator=MulticlassClassificationEvaluator(labelCol='label', metricName='f1'),\n        numFolds=5,\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(seed=3, numTrees=5, featuresCol=\"features\", labelCol=\"label\")\n\nrfc_param = ParamGridBuilder() \\\n            .addGrid(rfc.numTrees, [5, 10, 15]) \\\n            .build()\n\nrfc_model = build_model(rfc, rfc_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrfc_fit_model = rfc_model.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_pred = rfc_fit_model.transform(test)\n\nrfc_pred.select(\"prediction\").dropDuplicates().collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\nrfc_f1_score = evaluator.evaluate(rfc_pred, {evaluator.metricName: \"f1\"})\nprint(\"f1: {}\".format(rfc_f1_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_fit_model.bestModel.stages[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_feature_importance_df = pd.DataFrame()\nrfc_feature_importance_df['features'] = cat_cols + num_cols\nrfc_feature_importance_df['importance'] = rfc_fit_model.bestModel.stages[-1].featureImportances.values.tolist()\nrfc_feature_importance_df = rfc_feature_importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\nrfc_feature_importance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.barplot(x='importance', y='features', data=rfc_feature_importance_df, color='steelblue')\nplt.title('Feature Importance')\nplt.ylabel('');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top feature importances:\n* days after registration\n* setting-checking events per hour\n* upgrade-related events per hour\n* ads watched per hour\n* songs played per hour"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}