{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Churn Prediction with PySpark on AWS EMR"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# import libraries\nimport time\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = None\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, LongType, TimestampType\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# set up a Spark session\nspark = SparkSession \\\n        .builder \\\n        .appName('Sparkify Churn Prediction') \\\n        .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def load_data(path):\n    '''\n    Load a data set that is stored under a given path. \n    Rows with missing ids and duplicate rows will be removed.\n    '''\n    \n    # load small subset for feature engineering\n    df = spark.read.json(path)\n    \n    # split smaller dataset to work with\n    df, _ = df.randomSplit([0.1, 0.9], 42)\n    \n    # select features we are going to deal with\n    df = df.select(['userId', 'registration', 'ts', 'page'])\n\n    # drop rows with missing user id\n    df = df.where(df.userId != '')\n    \n    # drop duplicate rows if any exists\n    df = df.dropDuplicates()\n    \n    # count number of rows\n    print(f'Dataset have {df.count()} rows')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def feature_creation(df):\n    '''\n    The following Features will be creted at user level\n    Afterwards, all features are joined, checked for multicollinearity and strongly correlating features will be removed.\n    '''\n    \n    # use the Cancellation Confirmation event to define churn\n    churned_users = df.filter(F.col('page')=='Cancellation Confirmation')\n    print(f'Number of churned users in dataset {churned_users.count()}')\n    flag_churn = F.udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n    df = df.withColumn('churn', flag_churn('page'))\n    \n    # page-level features\n    get_page = F.udf(lambda x: 'page_' + x.replace(' ', '_').lower())\n    include_page = ['Roll Advert', 'Settings', 'Thumbs Down', 'Error', 'Upgrade', 'About']\n    page = df.filter(df['page'].isin(include_page)).withColumn('page', get_page(df['page']))\\\n            .groupBy(['userId']).pivot('page').agg(F.count('page')).fillna(0)\n    \n    # convert Timestamps (ts) to Datetime\n    df = df.withColumn('reg_date', (F.col('registration')/1000).cast(TimestampType()))\n    df = df.withColumn('date', (F.col('ts')/1000).cast(TimestampType()))\n\n    # user-based observation start/end dates\n    min_date = df.agg({'date':'min'}).collect()[0]['min(date)']\n    max_date = df.agg({'date':'max'}).collect()[0]['max(date)']\n    min_reg_date = df.agg({'reg_date':'min'}).collect()[0]['min(reg_date)']\n    max_reg_date = df.agg({'reg_date':'max'}).collect()[0]['max(reg_date)']\n\n    # get first log date\n    w = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n    df = df.withColumn('first_date', F.first('date').over(w))\n\n    # infer observation start date\n    df = df.withColumn('obs_start',\n                      (F.when(F.col('reg_date')<min_date, min_date)\n                        .when(F.col('reg_date')<F.col('first_date'), F.col('reg_date'))\n                        .otherwise(F.col('first_date')))\n                      )\n\n    # infer observation end date\n    df = df.withColumn('obs_end',\n                      (F.when(F.last('churn').over(w)==1, F.last('date').over(w))\n                         .otherwise(max_date))\n                      )\n\n    # aggregation by user\n    df = df.groupby('userId').agg(\n\n        # User-level features\n        F.max('churn').alias('churn'),\n        F.first('reg_date').alias('reg_date'),\n        F.first('obs_start').alias('obs_start'),\n        F.first('obs_end').alias('obs_end'),\n        )\n    \n    # user_lifetime\n    df = df.withColumn('user_lifetime', F.datediff('obs_end', 'reg_date')).select(['userId', 'user_lifetime', 'churn'])\n    \n    # bringing all together\n    df = df.join(page, ['userId'], how='outer').fillna(0)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def feature_scaling(df):\n    '''\n    Perform feature scaling on a set of given data . \n    '''\n    \n    # get features without ids and labels\n    feature_cols = df.drop('userId', 'churn').columns\n    print(str(len(feature_cols)) + ' Features for Scaling:\\n')\n    print(feature_cols)\n\n    # Vector assembler\n    assembler = VectorAssembler(inputCols=feature_cols, outputCol='FeatureVector')\n    \n    # PySpark.ML expects the target column to be named as 'labelCol' af data type double\n    df = df.withColumn('label', df['churn'].cast('float')).drop('churn')\n    \n    # feature scaler    \n    scaler = MinMaxScaler(inputCol='FeatureVector', outputCol='ScaledFeatures')\n    \n    # perform assembling and standardizing of features\n    df = assembler.transform(df)\n    scalerModel = scaler.fit(df)\n    scaled_df = scalerModel.transform(df)\n    \n    # just take scaled feature vector and labels\n    scaled_df = scaled_df.select('ScaledFeatures', 'label')\n    # print('\\nReduced to scaled feature vector and labels:\\n')\n    # print(scaled_df.printSchema())\n    # print(scaled_df.head(1))\n    \n    return scaled_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def train_test_split(scaled_df):\n    '''\n    Split the data into training set and test set. \n    '''\n    \n    # Train-test split\n    ratio = 0.9\n    train_set = scaled_df.drop('userId').sampleBy('label', fractions={0:ratio, 1:ratio}, seed=42)\n    test_set = scaled_df.drop('userId').subtract(train_set)\n    \n    # show number of rows in train and test sets\n    print(train_set.count(), test_set.count())\n    return train_set, test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def cross_validation(classifier, parameter_grid):\n    '''\n    Performs K-fold cross validation for estimating the performance of a predictive model\n    '''\n    \n    crossval = CrossValidator(estimator = classifier,\n                              estimatorParamMaps = parameter_grid,\n                              evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n                              numFolds = 3)\n    \n    return crossval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def train_model(classifier, train_set, parameter_grid):\n    '''\n    Training of classification model\n    '''\n    \n    # cross validation\n    crossval = cross_validation(classifier, parameter_grid)\n    \n    # train model\n    model = crossval.fit(train_set)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def evaluate_model(classifier, data):\n    '''\n    Evaluation of trained classification model\n    '''\n    \n    # predict\n    predictions = classifier.transform(data)\n    \n    # evaluator\n    evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label')\n    \n    # calculate metrics\n    metrics = {}\n    metrics['precision'] = evaluator.evaluate(predictions, {evaluator.metricName: 'weightedPrecision'})\n    metrics['recall'] = evaluator.evaluate(predictions, {evaluator.metricName: 'weightedRecall'})\n    metrics['f1'] = evaluator.evaluate(predictions, {evaluator.metricName: 'f1'})\n    metrics['accuracy'] = evaluator.evaluate(predictions, {evaluator.metricName: 'accuracy'})\n    \n    return metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def ml_pipeline(model, path):\n    '''\n    Machine Learning pipeline that brings all steps together: \n    '''\n    \n    print('\\nModel: ' + model + '\\n')\n    \n    print('----------------------------')\n    print('1. Load Data')\n    print('----------------------------')\n    \n    df = load_data(path)\n    \n    print('----------------------------')\n    print('2. Feature Creation')\n    print('----------------------------')\n    \n    df = feature_creation(df)\n\n    print('----------------------------')\n    print('3. Feature Scaling')\n    print('----------------------------')\n    \n    scaled_df = feature_scaling(df)\n    \n    print('----------------------------')\n    print('4. Train-Test Split')\n    print('----------------------------')\n    \n    train_set, test_set = train_test_split(scaled_df)\n    \n    print('----------------------------')\n    print('5. Build model')\n    print('----------------------------')\n\n    if model == 'Random Forest':\n        classifier = RandomForestClassifier(labelCol='label', featuresCol='ScaledFeatures')\n        file_name = 'cv_randomforest_cf.model'\n        \n    print('----------------------------')\n    print('6. & 7. Cross Val. & Training')\n    print('----------------------------')\n    \n    parameter_grid = ParamGridBuilder() \\\n                    .addGrid(classifier.numTrees, [5, 7, 10]).build()\n    \n    trained_classifier = train_model(classifier, train_set, parameter_grid)\n    \n    # Get best model\n    bestModel = trained_classifier.bestModel\n    print(bestModel)\n    \n    # for saving the best trained model uncomment:\n    bestModel.write().overwrite() #.save(file_name)\n    \n    print('----------------------------')\n    print('8. Model Evaluation')\n    print('----------------------------')\n    \n    metrics = evaluate_model(trained_classifier, test_set)\n    print(metrics)\n    \n    return trained_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Random Forest Classifier\n\ncv_randomforest_cf = ml_pipeline('Random Forest', 's3n://udacity-dsnd/sparkify/sparkify_event_data.json')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}